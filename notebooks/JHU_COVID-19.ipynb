{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Novel Coronavirus (SARS-CoV-2) and COVID-19 Unpivoted Data\n",
    "\n",
    "The following script takes data from the repository of the 2019 Novel Coronavirus Visual Dashboard operated by Johns Hopkins University's Center for Systems Science and Engineering (JHU CSSE). It will apply necessary cleansing/reformatting to make it use in traditional relational databases and data visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pycountry\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "output_folder = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded directly from Johns Hopkins git repository, located at: https://github.com/CSSEGISandData/COVID-19. Their repository has three different CSV files â€“ one each for `confirmed`, `deaths` and `recovered` data. The data is keyed into an array of `pandas` `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv\",keep_default_na=False)\n",
    "deaths = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv\",keep_default_na=False)\n",
    "recovered = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv\",keep_default_na=False)\n",
    "\n",
    "confirmed[\"Case_Type\"] = \"Confirmed\"\n",
    "deaths[\"Case_Type\"] = \"Deaths\"\n",
    "recovered[\"Case_Type\"] = \"Recovered\"\n",
    "\n",
    "key_columns = [\"Country/Region\",\n",
    "               \"Province/State\",\n",
    "               \"Lat\",\n",
    "               \"Long\",\n",
    "               \"Case_Type\"]\n",
    "\n",
    "data = [confirmed, deaths, recovered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset stores the number of `Cases` for a given day in columns. \n",
    "This is not useful for reporting, thus we move these date columns to rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(df):\n",
    "    # unpivot all non-key columns\n",
    "    melted = df.melt(id_vars=key_columns, var_name=\"Date\", value_name=\"Cases\")\n",
    "    # change our new Date field to Date type\n",
    "    melted[\"Date\"]= pd.to_datetime(melted[\"Date\"]) \n",
    "    \n",
    "    return melted\n",
    "\n",
    "unpivoted_data = list(map(unpivot, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are recombining the data set first to use county-level data aggregates before 09 March and state-level data thereafter. Because the data for `US-VI` (U.S. Virgin Islands) still contains a comma in data after March 10, we are executing a substitution before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_incorrect_county_state_data(df):\n",
    "    stateBeforeMarch9th = df[ (df[\"Date\"] <= \"2020-03-09\") & (df[\"Country/Region\"] == \"US\") & (df[\"Province/State\"].str.contains(\",\") == False) ].index\n",
    "    countyAfterMarch10th = df[ (df[\"Date\"] > \"2020-03-09\") & (df[\"Country/Region\"] == \"US\") & df[\"Province/State\"].str.contains(\",\") ].index\n",
    "\n",
    "    return df.drop(stateBeforeMarch9th).drop(countyAfterMarch10th)\n",
    "\n",
    "unpivoted_data = [df.replace({\"Virgin Islands, U.S.\": \"Virgin Islands\"}) for df in unpivoted_data]\n",
    "unpivoted_data = [drop_incorrect_county_state_data(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize data on the Virgin Islands and Washington D.C. in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locality_replacements = {\"Washington, D.C.\": \"District of Columbia\",\n",
    "                         \"Virgin Islands, U.S.\": \"Virgin Islands, VI\"}\n",
    "\n",
    "def replace_localities(df):\n",
    "    return df.replace(locality_replacements)\n",
    "\n",
    "unpivoted_data = [replace_localities(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we resolve the geographies of U.S. counties to their respective states (`Province/State`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivisions = {i.name: i.code for i in pycountry.subdivisions.get(country_code=\"US\")}\n",
    "abbreviations = {subdivisions[k]: k for k in subdivisions}\n",
    "\n",
    "def resolve_US_geography(row):\n",
    "    county, state = row[\"Province/State\"].split(\", \")\n",
    "    state.replace(\"D.C.\", \"DC\")\n",
    "    row[\"Province/State\"] = abbreviations[\"US-\" + state.strip()]\n",
    "    return row\n",
    "        \n",
    "def resolve_geography_df(df):\n",
    "    return df.apply(lambda row: resolve_US_geography(row) if row[\"Country/Region\"] == \"US\" and row[\"Province/State\"] not in list(subdivisions.keys()) and \", \" in row[\"Province/State\"] else row, axis=\"columns\")\n",
    "\n",
    "unpivoted_data = [resolve_geography_df(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of states have inconsistent naming or special characters, such as `Taiwan*`. These are normalised through a replacement `dict`. Data is then aggregated for each division by date and case type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_names = {\n",
    "    \"Holy See\": \"Vatican City\",\n",
    "    \"Hong Kong SAR\": \"Hong Kong\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Macao SAR\": \"Macau\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Republic of Moldova\": \"Moldova\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Saint Martin\": \"St. Martin\",\n",
    "    \"Taipei and environs\": \"Taiwan\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"occupied Palestinian territory\": \"Palestine\",\n",
    "    \"Taiwan*\": \"Taiwan\",\n",
    "    \"Congo (Brazzaville)\": \"Republic of the Congo\"\n",
    "}\n",
    "\n",
    "\n",
    "for idx,df in enumerate(unpivoted_data):\n",
    "    df[\"Country/Region\"] = df[\"Country/Region\"].replace(changed_names)\n",
    "    df[\"Cases\"] = df[\"Cases\"].replace('',0).astype(int)\n",
    "        \n",
    "    unpivoted_data[idx] = df.groupby(by=[\"Country/Region\",\"Province/State\",\"Date\",\"Case_Type\"], as_index=False) \\\n",
    "        .agg({\"Cases\": \"sum\", \"Long\": \"first\", \"Lat\": \"first\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating case changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the data by primary keys and `Date` to ensure we can add a `Differences` column as a window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = list(map(lambda df: df.sort_values(by=key_columns + [\"Date\"], ascending=True), unpivoted_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `Cases` are actual snapshots (running numbers), we define changes as the difference to the previous day's value. In other words, `Difference` equals today's `Cases` minus yesterday's `Cases` for each region/state and each case category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in sorted_data:\n",
    "    df[\"Difference\"] = df[\"Cases\"] - df.groupby( key_columns )[\"Cases\"].shift(1, fill_value = 0) \n",
    "\n",
    "concated = pd.concat(sorted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating active cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acive cases are confirmed cases that are not deceased or registered as recovered. This is relevant as active cases determine the demands on the healthcare system. We calculate `Active` case types as:\n",
    "\n",
    "```\n",
    "Active = Confirmed - Deaths - Recovered\n",
    "```\n",
    "\n",
    "As a first step, we merge the different type of cases into a single row for each `Country/Province/Date` keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = concated[concated[\"Case_Type\"].eq(\"Confirmed\")]\n",
    "deaths = concated[concated[\"Case_Type\"].eq(\"Deaths\")]\n",
    "recovered = concated[concated[\"Case_Type\"].eq(\"Recovered\")]\n",
    "\n",
    "active = confirmed  \\\n",
    "        .merge(deaths, validate= \"one_to_one\", suffixes =[\"\",\"_d\"], on=[\"Country/Region\",\"Province/State\",\"Date\"]) \\\n",
    "        .merge(recovered, validate= \"one_to_one\", suffixes =[\"\",\"_r\"], on= [\"Country/Region\",\"Province/State\",\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply the calculations both for `Cases` and `Difference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active[\"Case_Type\"] = \"Active\"\n",
    "active[\"Cases\"] = active[\"Cases\"] - active[\"Cases_r\"] - active[\"Cases_d\"]\n",
    "active[\"Difference\"] = active[\"Difference\"] - active[\"Difference_r\"] - active[\"Difference_d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the `Active` segment with the original one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([concated,active], join=\"inner\")\n",
    "\n",
    "data[\"Case_Type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save the file locally, we add the `Last_Update_Date` in `UTC` time zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Last_Update_Date\"] = datetime.utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the output in the `output` folder as `JHU_COVID-19.csv` as an unindexed CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(output_folder + \"JHU_COVID-19.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
